# OpenAI-Compatible API Configuration Example
# This configuration uses a generic OpenAI-compatible API as the primary provider

# Required API Keys and Configuration
OPENAI_COMPATIBLE_API_KEY="your-api-key"
OPENAI_COMPATIBLE_BASE_URL="https://api.example.com/v1"
OPENAI_API_KEY="your-openai-key" # Needed for fallback

# Optional: Other provider keys
# ANTHROPIC_API_KEY="your-anthropic-api-key"
# GEMINI_API_KEY="your-google-ai-studio-key"
# OPENROUTER_API_KEY="sk-or-v1-..."
# DEEPSEEK_API_KEY="sk-..."

# Provider Configuration
PREFERRED_PROVIDER="openai-compatible"
BIG_MODEL="your-big-model-name"
SMALL_MODEL="your-small-model-name"

# Examples for different OpenAI-compatible services:

# Example 1: Local Ollama setup
# OPENAI_COMPATIBLE_BASE_URL="http://localhost:11434/v1"
# BIG_MODEL="llama3.1:70b"
# SMALL_MODEL="llama3.1:8b"

# Example 2: Together.ai
# OPENAI_COMPATIBLE_BASE_URL="https://api.together.xyz/v1"
# BIG_MODEL="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
# SMALL_MODEL="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"

# Example 3: Groq
# OPENAI_COMPATIBLE_BASE_URL="https://api.groq.com/openai/v1"
# BIG_MODEL="llama-3.1-70b-versatile"
# SMALL_MODEL="llama-3.1-8b-instant"

# Usage Examples:
# Start the proxy: uv run uvicorn server:app --host 0.0.0.0 --port 8083
# Use with Claude Code: ANTHROPIC_BASE_URL="http://localhost:8083" ANTHROPIC_API_KEY="DUMMY" claude
# 
# Model mapping:
# - claude-3-haiku → openai-compatible/your-small-model-name
# - claude-3-sonnet → openai-compatible/your-big-model-name
# - Direct access: openai-compatible/your-model-name
